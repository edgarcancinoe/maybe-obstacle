{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "044aa663",
      "metadata": {
        "id": "044aa663"
      },
      "source": [
        "# Uncertainty-Aware Road Obstacle Identification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e16c6661",
      "metadata": {
        "id": "e16c6661"
      },
      "source": [
        "## 1. Imports\n",
        "Import necessary libraries like PyTorch, torchvision, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5fe48609",
      "metadata": {
        "id": "5fe48609"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import Cityscapes     # ready-made dataset wrapper\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib, shutil, zipfile\n",
        "from tqdm import tqdm # Import tqdm\n",
        "from torch.utils.data import Subset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6da946b4",
      "metadata": {
        "id": "6da946b4"
      },
      "source": [
        "## 2. Globals\n",
        "Define global variables such as paths, batch size, learning rate, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2556745f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2556745f",
        "outputId": "4e7116db-4173-4701-bec6-d5d1437a9b1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CPU\n"
          ]
        }
      ],
      "source": [
        "# Global variables\n",
        "DATASET_PATH = \"./datasets/\"\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 50\n",
        "\n",
        "# Select the best available device (CPU, CUDA, or MPS (Metal Performance Shaders for macOS))\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"Using MPS (Metal Performance Shaders) device\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "DEVICE = device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfa362e7",
      "metadata": {
        "id": "bfa362e7"
      },
      "source": [
        "## 3. Utils\n",
        "Helper functions for visualization, preprocessing, and more."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "be5ae29f",
      "metadata": {
        "id": "be5ae29f"
      },
      "outputs": [],
      "source": [
        "\n",
        "def visualize_sample(image, mask):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(\"Image\")\n",
        "    plt.imshow(image)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.title(\"Mask\")\n",
        "    plt.imshow(mask)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unzip(zip_path,dest):\n",
        "  print(\"Extracting {zippath.name} ...\")\n",
        "  with zipfile.ZipFile(zip_path) as z:\n",
        "    z.extractall(dest)\n",
        "  print(\"done!\")"
      ],
      "metadata": {
        "id": "Z7fNd1QXMt4g"
      },
      "id": "Z7fNd1QXMt4g",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b07190be",
      "metadata": {
        "id": "b07190be"
      },
      "source": [
        "### 3.1 Utils for Conformal Risk Control"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bec61ef6",
      "metadata": {
        "id": "bec61ef6"
      },
      "source": [
        "Miscoverage Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ac3beb80",
      "metadata": {
        "id": "ac3beb80"
      },
      "outputs": [],
      "source": [
        "def miscoverage_loss(set_mask: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Calculate the miscoverage loss for a set prediction.\n",
        "\n",
        "    Args:\n",
        "        set_mask: A binary tensor of shape [C, H, W] where set_mask[k,i,j] == 1\n",
        "                  if class k is in the predicted set for pixel (i,j).\n",
        "        labels: A tensor of shape [H, W] with ground-truth class IDs.\n",
        "\n",
        "    Returns:\n",
        "        A scalar tensor representing the miscoverage rate (proportion of\n",
        "        pixels where the true label is not in the predicted set),\n",
        "        ignoring pixels with invalid labels.\n",
        "    \"\"\"\n",
        "    C, H, W = set_mask.shape\n",
        "    # Create a mask for valid labels (class IDs within the range [0, C-1])\n",
        "    valid_pixel_mask = (labels >= 0) & (labels < C)\n",
        "\n",
        "    # Create a modified labels tensor where invalid labels are temporarily set to a valid index (e.g., 0)\n",
        "    # This prevents scatter_ from failing on invalid indices like 255.\n",
        "    # Ensure the temporary index is within the valid range [0, C-1]. 0 is a safe choice if it's always a valid class ID.\n",
        "    # If 0 is not guaranteed to be a valid class ID in the dataset, you might need a different placeholder,\n",
        "    # but for Cityscapes 0 ('road') is usually valid.\n",
        "    placeholder_label = 0 # A valid class index to use temporarily\n",
        "    masked_labels = torch.where(valid_pixel_mask, labels, torch.tensor(placeholder_label, device=labels.device, dtype=labels.dtype))\n",
        "\n",
        "    # Create a ground-truth mask using the masked labels\n",
        "    gt_mask = torch.zeros_like(set_mask)\n",
        "    # Scatter values using the masked_labels tensor\n",
        "    gt_mask.scatter_(0, masked_labels.unsqueeze(0), 1)     # in-place one-hot\n",
        "\n",
        "    # Pixel is covered if gt class ∈ predicted set\n",
        "    # This calculation includes pixels with invalid labels temporarily set to 0 in gt_mask\n",
        "    covered = (set_mask * gt_mask).sum(dim=0) # covered is [H, W], 1 if true class in set, 0 otherwise\n",
        "\n",
        "    # Now, apply the valid_pixel_mask to exclude contributions from invalid pixels\n",
        "    covered = covered * valid_pixel_mask.float()\n",
        "\n",
        "    # Calculate total number of valid pixels\n",
        "    num_valid_pixels = valid_pixel_mask.sum().float()\n",
        "\n",
        "    if num_valid_pixels == 0:\n",
        "        # If there are no valid pixels, the miscoverage is undefined or can be considered 0.0\n",
        "        # Returning 0.0 is a reasonable default in this context.\n",
        "        return torch.tensor(0.0, device=set_mask.device)\n",
        "\n",
        "    # Calculate coverage ratio only over valid pixels\n",
        "    coverage = covered.sum() / num_valid_pixels\n",
        "\n",
        "    # Miscoverage is 1 - coverage\n",
        "    return 1.0 - coverage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7108835c",
      "metadata": {
        "id": "7108835c"
      },
      "source": [
        "Least Ambiguous Set-Valued Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "90e4bf02",
      "metadata": {
        "id": "90e4bf02"
      },
      "outputs": [],
      "source": [
        "def T_lambda(probs, lam):\n",
        "    # probs: [C, H, W], lam: float ∈ [0,1]\n",
        "    return (probs >= (1 - lam)).float()  # binariza por canal"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98f1814c",
      "metadata": {
        "id": "98f1814c"
      },
      "source": [
        "Dichotomic search over the parameter λ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b3577ba5",
      "metadata": {
        "id": "b3577ba5"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "We can compute the optimal like this because the Loss is monotonic in lambda\n",
        "-  Lambda is the threshold for binarization in T_lambda\n",
        "-  Alpha is the tolerated risk level\n",
        "-  Eps is the precision for the binary search\n",
        "\"\"\"\n",
        "\n",
        "def binary_search_lambda(probs_list,labels_list,alpha=0.1,eps=1e-3):\n",
        "    left , right = 0.0, 1.0\n",
        "    while right - left > eps:\n",
        "        mid = (left + right) / 2.0\n",
        "        print(mid)\n",
        "\n",
        "        risk_sum = 0.0\n",
        "        for probs,labels in zip(probs_list,labels_list):\n",
        "            mask = T_lambda(probs, mid)\n",
        "            risk_sum += miscoverage_loss(mask,labels)\n",
        "        avr_risk += risk_sum / len(probs_list)\n",
        "        if avr_risk <= alpha:\n",
        "            right = mid\n",
        "        else:\n",
        "            left = mid\n",
        "    return mid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def online_risk_lambda(lam,loader):\n",
        "  \"We calculate the risk online so the RAM does not fills up\"\n",
        "  risk_sum, n = 0.0,0.0\n",
        "  with torch.no_grad():\n",
        "    for img, gt in tqdm(loader, desc=f\"Calculating risk for lambda={lam:.4f}\"):\n",
        "      img = img.to(device)\n",
        "      gt = gt.squeeze(0)\n",
        "      out = model(img)['out'][0]\n",
        "      probs = torch.softmax(out,dim=0).cpu()\n",
        "      mask = T_lambda(probs,lam)\n",
        "      risk_sum += miscoverage_loss(mask,gt)\n",
        "      n +=1\n",
        "      del img, out, probs,mask\n",
        "      torch.cuda.empty_cache()\n",
        "    avg_risk = risk_sum / n\n",
        "    return avg_risk"
      ],
      "metadata": {
        "id": "RaZW81IDZ8Cc"
      },
      "id": "RaZW81IDZ8Cc",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def online_binary_search(model, loader, alpha=0.1, eps=1e-3, device='cuda'):\n",
        "    # Búsqueda binaria\n",
        "    left, right = 0.0, 1.0\n",
        "    while right - left > eps:\n",
        "        mid   = (left + right) / 2\n",
        "        r_mid = online_risk_lambda(mid,loader)\n",
        "        if r_mid <= alpha:\n",
        "            right = mid\n",
        "        else:\n",
        "            left  = mid\n",
        "    return (left + right) / 2"
      ],
      "metadata": {
        "id": "l35HzeJzdHpv"
      },
      "id": "l35HzeJzdHpv",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0d8ba0e1",
      "metadata": {
        "id": "0d8ba0e1"
      },
      "source": [
        "## 4. Data\n",
        "Load and preprocess datasets like Cityscapes, LostAndFound, and Fishyscapes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b06c9692",
      "metadata": {
        "id": "b06c9692"
      },
      "outputs": [],
      "source": [
        "# Code for data loading and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0579cbaf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0579cbaf",
        "outputId": "6b1680b8-d01e-4fc0-d9d1-2c7da89a752a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "ZIPs found in Drive\n"
          ]
        }
      ],
      "source": [
        "#@title Retrieving CityScapes from Drive\n",
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Path where the two ZIPs are uploaded\n",
        "GDRIVE_CS = pathlib.Path(\"/content/drive/MyDrive/datasets/cityscapes\")\n",
        "\n",
        "assert (GDRIVE_CS / 'leftImg8bit_trainvaltest.zip').exists(), \"Missing ZIP in Drive.\"\n",
        "assert (GDRIVE_CS / 'gtFine_trainvaltest.zip').exists(), \"Missing ZIP in Drive.\"\n",
        "print(\"ZIPs found in Drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Copying and extracting to VM\n",
        "#Lets copy them to the VM (if they are not already there)\n",
        "COLAB_CS = pathlib.Path('/content/cityscapes')\n",
        "assert not (COLAB_CS / 'leftImg8bit_trainvaltest.zip').exists(), \"Zip file already in VM.\"\n",
        "assert not (COLAB_CS / 'gtFine_trainvaltest.zip').exists(), \"Zip file already in VM.\"\n",
        "\n",
        "COLAB_CS.mkdir(parents=True, exist_ok=True)\n",
        "shutil.copy(GDRIVE_CS / 'leftImg8bit_trainvaltest.zip', '/content/')\n",
        "shutil.copy(GDRIVE_CS / 'gtFine_trainvaltest.zip', '/content/')\n",
        "\n",
        "print(\"ZIP files copied from GDrive\")\n",
        "\n",
        "for zname in ('leftImg8bit_trainvaltest.zip','gtFine_trainvaltest.zip'):\n",
        "  zippath = pathlib.Path('/content') / zname\n",
        "  unzip(zippath, COLAB_CS)\n",
        "  zippath.unlink()  #delete zip file\n",
        "  print(\"ZIP files extracted and deleted\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDNBjDHkHALV",
        "outputId": "549b4ca6-782d-4af7-bfe1-ccc6308c0340"
      },
      "id": "CDNBjDHkHALV",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP files copied from GDrive\n",
            "Extracting {zippath.name} ...\n",
            "done!\n",
            "ZIP files extracted and deleted\n",
            "Extracting {zippath.name} ...\n",
            "done!\n",
            "ZIP files extracted and deleted\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset + DataLoader\n",
        "\n",
        "# Convert the RGB PIL image (H,W,3 uint8) into a float32 tensor in [0,1]\n",
        "img_tf = transforms.ToTensor()\n",
        "# Ground-truth masks come in as PIL “mode =L” images (H,W uint8). We need them as int64 tensors for loss/calibration, so:\n",
        "# lbl_tf = transforms.Lambda(\n",
        "#     lambda pil_img: torch.as_tensor(np.array(pil_img), dtype=torch.long)\n",
        "# )\n",
        "\n",
        "# 1. First define the label mapping (34 → 19 classes)\n",
        "# This is the official Cityscapes mapping:\n",
        "cityscapes_train_ids = {\n",
        "    0: 255, 1: 255, 2: 255, 3: 255, 4: 255, 5: 255, 6: 255,\n",
        "    7: 0, 8: 1, 9: 255, 10: 255, 11: 2, 12: 3, 13: 4,\n",
        "    14: 255, 15: 255, 16: 255, 17: 5, 18: 255, 19: 6,\n",
        "    20: 7, 21: 8, 22: 9, 23: 10, 24: 11, 25: 12, 26: 13,\n",
        "    27: 14, 28: 15, 29: 255, 30: 255, 31: 16, 32: 17, 33: 18\n",
        "}\n",
        "\n",
        "def convert_to_train_ids(pil_img):\n",
        "    # Convert PIL to numpy array\n",
        "    labels = np.array(pil_img)\n",
        "    # Vectorized mapping using numpy\n",
        "    train_ids = np.vectorize(cityscapes_train_ids.get)(labels)\n",
        "    return torch.as_tensor(train_ids, dtype=torch.long)\n",
        "\n",
        "\n",
        "# ready-made dataset wrapper\n",
        "val_set = Cityscapes(\n",
        "    root=str(COLAB_CS),\n",
        "    split = 'val',            # 500 fine labelled images\n",
        "    mode='fine',              # using the accurate (not coarse) masks\n",
        "    target_type ='semantic',  # we want a single channel with class-IDs\n",
        "    transform=img_tf,         # apply to the RGB image\n",
        "    # target_transform=lbl_tf   # apply to the mask\n",
        "    target_transform = convert_to_train_ids\n",
        ")\n",
        "\n",
        "#Randomly select 100 indices\n",
        "num_calibration = 50\n",
        "indices = torch.randperm(len(val_set))[:num_calibration]\n",
        "\n",
        "#Create Subset\n",
        "calibration_set = Subset(val_set,indices)\n",
        "# Verify\n",
        "print(f\"Calibration set size: {len(calibration_set)}\")  # Should be 100\n",
        "\n",
        "#Wraping in dataloader\n",
        "calibration_loader = DataLoader(\n",
        "    calibration_set,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader =torch.utils.data.DataLoader(\n",
        "    val_set,\n",
        "    batch_size=1,            # one image per step\n",
        "    shuffle=False,           # no randomise\n",
        "    num_workers=2,           # 2 background workers decode PNGs in parallel\n",
        "    pin_memory=True          # speed-up host→GPU transfer when using CUDA\n",
        ")\n",
        "\n",
        "#Sanity Check\n",
        "assert len(val_set) == 500, \"Sanity Check  unsuccesful. Val set should have 500 items\"\n",
        "sample = val_set[0]\n",
        "print(\"Unique labels:\", torch.unique(sample[1]))  # Should show 0-18 + 255"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ov1YQknsHiAJ",
        "outputId": "f191c3f4-2a67-4403-c724-f5ae1edd54da"
      },
      "id": "ov1YQknsHiAJ",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calibration set size: 50\n",
            "Unique labels: tensor([  0,   1,   2,   5,   6,   7,   8,  10,  11,  12,  13,  17,  18, 255])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e70ceb8f",
      "metadata": {
        "id": "e70ceb8f"
      },
      "source": [
        "## 5. Network\n",
        "Define the neural network architecture for semantic segmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9f5edcde",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9f5edcde",
        "outputId": "66b0ca37-5456-4050-e74c-189773024dfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 152MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeepLabV3(\n",
              "  (backbone): IntermediateLayerGetter(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): DeepLabHead(\n",
              "    (0): ASPP(\n",
              "      (convs): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU()\n",
              "        )\n",
              "        (1): ASPPConv(\n",
              "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU()\n",
              "        )\n",
              "        (2): ASPPConv(\n",
              "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU()\n",
              "        )\n",
              "        (3): ASPPConv(\n",
              "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU()\n",
              "        )\n",
              "        (4): ASPPPooling(\n",
              "          (0): AdaptiveAvgPool2d(output_size=1)\n",
              "          (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (3): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (project): Sequential(\n",
              "        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): ReLU()\n",
              "    (4): Conv2d(256, 19, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Define the model architecture here\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# Load a pretrained DeepLabV3 with ResNet-50 backbone\n",
        "# ------------------------------------------------------------------\n",
        "model = models.segmentation.deeplabv3_resnet50(pretrained=False, num_classes=19).to(device)                         # move network to the selected device\n",
        "model.eval()                                 # inference mode (deactivate dropout/BN updates)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "685e0300",
      "metadata": {
        "id": "685e0300"
      },
      "source": [
        "### Conformal Risk Control\n",
        "\n",
        "#### Implementation\n",
        "1. Pass the test set through the model to get logits and compute softmax\n",
        "2. Use optimal Lambda to get mask\n",
        "3. Visualize the cardinality of the set. (Pixel wise anomaly detection measurement)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lambda_hat = online_binary_search(model, calibration_loader, alpha=0.9)\n",
        "print(f\"λ̂ obtenido: {lambda_hat:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "PrNrLIWGV7JP",
        "outputId": "83d8fc38-f367-49fe-e8ff-186dcbc19aae"
      },
      "id": "PrNrLIWGV7JP",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating risk for lambda=0.5000: 100%|██████████| 50/50 [53:11<00:00, 63.83s/it]\n",
            "Calculating risk for lambda=0.7500:  88%|████████▊ | 44/50 [46:23<06:19, 63.26s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-193393496>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlambda_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monline_binary_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalibration_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"λ̂ obtenido: {lambda_hat:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-373596033>\u001b[0m in \u001b[0;36monline_binary_search\u001b[0;34m(model, loader, alpha, eps, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mleft\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmid\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mr_mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monline_risk_lambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mr_mid\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-3945633643>\u001b[0m in \u001b[0;36monline_risk_lambda\u001b[0;34m(lam, loader)\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m       \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m       \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT_lambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/segmentation/_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# contract: features is a dict of tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mout_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26c897d7",
      "metadata": {
        "id": "26c897d7"
      },
      "outputs": [],
      "source": [
        "# #@title Calibration\n",
        "# # 1. Pass the calibration set through the model to get logits\n",
        "# # 2. Convert logits to probabilities using softmax.\n",
        "# # 3. Run binary_search_lambda over the calibration probabilities to find the optimal Lambda\n",
        "\n",
        "\n",
        "# probs_list, labels_list = [],[]\n",
        "\n",
        "# with torch.no_grad():\n",
        "#       for img, gt in val_loader:\n",
        "#         img = img.to(device)                 # move image tensor to GPU/CPU\n",
        "#         logits = model(img)['out']           # dict → key 'out'; shape [1, C, H', W']\n",
        "#         # Convert logits to class-probabilities along channel dimension C\n",
        "#         probs = torch.softmax(logits[0], dim=0).cpu()  # remove batch dim\n",
        "#         # ----------- free everything that lives on GPU -----------\n",
        "#         del logits, img                         # drop GPU tensors\n",
        "#         torch.cuda.empty_cache()                # give the arena back to CUDA\n",
        "#         # ----------------------------------------------------------\n",
        "#         probs_list.append(probs)             # store for calibration / evaluation\n",
        "#         labels_list.append(gt.squeeze(0))    # remove batch dim → shape [H, W]\n",
        "\n",
        "# # At this point:\n",
        "# # probs_list[i] → torch.Tensor of shape [C, H, W] with per-class probabilities.\n",
        "# # labels_list[i] → torch.LongTensor shape [H, W] with ground-truth IDs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lambda_hat = binary_search_lambda(probs_list,labels_list,0.1,eps=1e-3)\n",
        "# # print(f\"λ̂ = {lambda_hat:.6f}\")\n"
      ],
      "metadata": {
        "id": "BHPh54_0SfcU"
      },
      "id": "BHPh54_0SfcU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tqdm import tqdm  # tqdm adds a smart progress-bar to any Python loop\n",
        "\n",
        "# # ----------------------------------------------------------\n",
        "# # Evaluate a loader with a *fixed* λ̂\n",
        "# # ----------------------------------------------------------\n",
        "# def evaluate(loader, lambda_hat):\n",
        "#     \"\"\"\n",
        "#     Prints the mean risk; should be ≤ alpha if the same split was\n",
        "#     used for calibration. If you run it on a fresh test set, this is\n",
        "#     an unbiased estimate of the true risk.\n",
        "#     \"\"\"\n",
        "#     risk_sum, n = 0.0, 0          # accumulate loss & sample count\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         # tqdm(...) wraps any iterator and shows a live progress-bar\n",
        "#         for img, gt in tqdm(loader, desc=\"Eval\"):\n",
        "#             img = img.to(device)                          # move RGB to GPU\n",
        "#             logits = model(img)['out']                    # forward pass\n",
        "#             probs  = torch.softmax(logits[0], dim=0).cpu()# C×H×W  ➜ CPU\n",
        "#             mask   = T_lambda(probs, lambda_hat)          # prediction set S_λ(x)\n",
        "\n",
        "#             # Your monotone loss (scalar tensor)\n",
        "#             risk   = miscoverage_loss(mask, gt.squeeze(0))\n",
        "#             risk_sum += risk.item()\n",
        "#             n += 1\n",
        "\n",
        "#     avg_risk = risk_sum / n\n",
        "#     print(f\"Average risk on evaluation split: {avg_risk:.4f}  (target α={alpha})\")\n"
      ],
      "metadata": {
        "id": "Sq4EF-xeXFnC"
      },
      "id": "Sq4EF-xeXFnC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e08b2d10",
      "metadata": {
        "id": "e08b2d10"
      },
      "source": [
        "## 6. Train\n",
        "Implement the training loop, including loss functions and optimizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec0ca15c",
      "metadata": {
        "id": "ec0ca15c"
      },
      "outputs": [],
      "source": [
        "# Training loop and loss computation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db0f6366",
      "metadata": {
        "id": "db0f6366"
      },
      "source": [
        "## 7. Test\n",
        "Test the trained model and evaluate performance metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "def3d94b",
      "metadata": {
        "id": "def3d94b"
      },
      "outputs": [],
      "source": [
        "# Evaluation metrics and performance analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa70fd84",
        "outputId": "49d05e9f-f410-4607-f123-801abaf466db"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "unique_class_ids = set()\n",
        "\n",
        "# Iterate through the validation dataset\n",
        "for _, labels in val_set:\n",
        "    # Convert the labels tensor to a NumPy array and find unique values\n",
        "    unique_ids = np.unique(labels.numpy())\n",
        "    # Add the unique IDs to the set\n",
        "    unique_class_ids.update(unique_ids)\n",
        "\n",
        "# Convert the set to a sorted list for better readability\n",
        "sorted_unique_class_ids = sorted(list(unique_class_ids))\n",
        "\n",
        "print(\"Unique class IDs found in the validation set:\")\n",
        "print(sorted_unique_class_ids)"
      ],
      "id": "fa70fd84",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique class IDs found in the validation set:\n",
            "[np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8), np.int64(9), np.int64(10), np.int64(11), np.int64(12), np.int64(13), np.int64(14), np.int64(15), np.int64(17), np.int64(18), np.int64(19), np.int64(20), np.int64(21), np.int64(22), np.int64(23), np.int64(24), np.int64(25), np.int64(26), np.int64(27), np.int64(28), np.int64(29), np.int64(30), np.int64(31), np.int64(32), np.int64(33)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af72592f"
      },
      "source": [
        "import torch\n",
        "from torchvision.transforms import InterpolationMode # Make sure this is imported if needed for previous steps\n",
        "from tqdm import tqdm # Import tqdm\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Monotone loss function (miscoverage indicator)\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "def miscoverage_loss(set_mask: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Calculate the miscoverage loss for a set prediction.\n",
        "\n",
        "    Args:\n",
        "        set_mask: A binary tensor of shape [C, H, W] where set_mask[k,i,j] == 1\n",
        "                  if class k is in the predicted set for pixel (i,j).\n",
        "        labels: A tensor of shape [H, W] with ground-truth class IDs.\n",
        "\n",
        "    Returns:\n",
        "        A scalar tensor representing the miscoverage rate (proportion of\n",
        "        pixels where the true label is not in the predicted set),\n",
        "        ignoring pixels with invalid labels.\n",
        "    \"\"\"\n",
        "    C, H, W = set_mask.shape\n",
        "    # Create a mask for valid labels (class IDs within the range [0, C-1])\n",
        "    valid_pixel_mask = (labels >= 0) & (labels < C)\n",
        "\n",
        "    # Create a modified labels tensor where invalid labels are temporarily set to a valid index (e.g., 0)\n",
        "    # This prevents scatter_ from failing on invalid indices like 255.\n",
        "    # Ensure the temporary index is within the valid range [0, C-1]. 0 is a safe choice if it's always a valid class ID.\n",
        "    # If 0 is not guaranteed to be a valid class ID in the dataset, you might need a different placeholder,\n",
        "    # but for Cityscapes 0 ('road') is usually valid.\n",
        "    placeholder_label = 0 # A valid class index to use temporarily\n",
        "    masked_labels = torch.where(valid_pixel_mask, labels, torch.tensor(placeholder_label, device=labels.device, dtype=labels.dtype))\n",
        "\n",
        "    # Create a ground-truth mask using the masked labels\n",
        "    gt_mask = torch.zeros_like(set_mask)\n",
        "    # Scatter values using the masked_labels tensor\n",
        "    gt_mask.scatter_(0, masked_labels.unsqueeze(0), 1)     # in-place one-hot\n",
        "\n",
        "    # Pixel is covered if gt class ∈ predicted set\n",
        "    # This calculation includes pixels with invalid labels temporarily set to 0 in gt_mask\n",
        "    covered = (set_mask * gt_mask).sum(dim=0) # covered is [H, W], 1 if true class in set, 0 otherwise\n",
        "\n",
        "    # Now, apply the valid_pixel_mask to exclude contributions from invalid pixels\n",
        "    covered = covered * valid_pixel_mask.float()\n",
        "\n",
        "    # Calculate total number of valid pixels\n",
        "    num_valid_pixels = valid_pixel_mask.sum().float()\n",
        "\n",
        "    if num_valid_pixels == 0:\n",
        "        # If there are no valid pixels, the miscoverage is undefined or can be considered 0.0\n",
        "        # Returning 0.0 is a reasonable default in this context.\n",
        "        return torch.tensor(0.0, device=set_mask.device)\n",
        "\n",
        "    # Calculate coverage ratio only over valid pixels\n",
        "    coverage = covered.sum() / num_valid_pixels\n",
        "\n",
        "    # Miscoverage is 1 - coverage\n",
        "    return 1.0 - coverage\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Set-valued prediction (thresholding function)\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "def T_lambda(probs: torch.Tensor, lam: float) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Construct a set-valued prediction S(x) by thresholding the probability\n",
        "    vector p(x).\n",
        "\n",
        "    Args:\n",
        "        probs: A tensor of shape [C, H, W] with per-class probabilities.\n",
        "        lam: The threshold parameter λ.\n",
        "\n",
        "    Returns:\n",
        "        A binary tensor of shape [C, H, W] where S[k,i,j] == 1 if\n",
        "        class k is included in the predicted set for pixel (i,j).\n",
        "    \"\"\"\n",
        "    # S_λ(x) = {k : p_k(x) ≥ λ}\n",
        "    return (probs >= lam).to(torch.uint8)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Risk function R(λ)\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "def online_risk_lambda(lam: float, loader: torch.utils.data.DataLoader, model: torch.nn.Module, device='cuda') -> float:\n",
        "    \"\"\"\n",
        "    Calculate the average miscoverage risk for a given lambda on a dataset\n",
        "    loaded by `loader`.\n",
        "    \"\"\"\n",
        "    risk_sum, n = 0.0, 0          # accumulate loss & sample count\n",
        "\n",
        "    # Ensure model is in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Wrap the loader with tqdm for a progress bar\n",
        "        for img, gt in tqdm(loader, desc=f\"Calculating risk for lambda={lam:.4f}\"):\n",
        "            img = img.to(device)                 # move image tensor to GPU/CPU\n",
        "            # Ensure gt is on the same device as labels will be processed\n",
        "            # gt = gt.to(device) # Keep gt on CPU as miscoverage_loss expects CPU tensor for labels\n",
        "            gt = gt.squeeze(0) # Remove batch dimension from ground truth\n",
        "\n",
        "            out = model(img)['out']           # dict → key 'out'; shape [1, C, H', W']\n",
        "            probs = torch.softmax(out[0],dim=0).cpu() # Move probabilities to CPU to save GPU memory\n",
        "            mask = T_lambda(probs,lam)\n",
        "\n",
        "            # Pass gt (on CPU) and mask (on CPU) to miscoverage_loss\n",
        "            risk_sum += miscoverage_loss(mask, gt).item() # Use .item() to get scalar value\n",
        "            n +=1\n",
        "\n",
        "            # Explicitly delete tensors and clear cache to manage memory\n",
        "            del img, out, probs, mask, gt\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    if n == 0:\n",
        "        return 0.0 # Return 0 risk if loader is empty\n",
        "\n",
        "    return risk_sum / n\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Binary search for the optimal lambda\n",
        "# ----------------------------------------------------------\n",
        "def online_binary_search(model: torch.nn.Module, loader: torch.utils.data.DataLoader, alpha: float, eps: float = 1e-3, device='cuda') -> float:\n",
        "    \"\"\"\n",
        "    Find the optimal lambda using binary search such that R(lambda) <= alpha.\n",
        "    \"\"\"\n",
        "    left, right = 0.0, 1.0 # Lambda is a probability, so it's between 0 and 1\n",
        "\n",
        "    print(f\"Starting binary search for lambda between {left} and {right} with target alpha={alpha} and eps={eps}\")\n",
        "\n",
        "    # Initial check for bounds (optional but good practice)\n",
        "    # Be cautious with calling online_risk_lambda multiple times if it's computationally expensive\n",
        "    # risk_at_0 = online_risk_lambda(left, loader, model, device)\n",
        "    # if risk_at_0 <= alpha:\n",
        "    #     print(f\"Risk at lambda=0 ({risk_at_0:.4f}) is <= alpha ({alpha}). Optimal lambda is 0.\")\n",
        "    #     return left\n",
        "\n",
        "    # risk_at_1 = online_risk_lambda(right, loader, model, device)\n",
        "    # if risk_at_1 > alpha:\n",
        "    #      print(f\"Warning: Even with lambda=1, risk ({risk_at_1:.4f}) is > alpha ({alpha}). Consider increasing alpha or checking model/data.\")\n",
        "         # The binary search will converge to 1.0 in this case.\n",
        "\n",
        "\n",
        "    iteration = 0\n",
        "    while right - left > eps:\n",
        "        mid   = (left + right) / 2\n",
        "        # print(f\"Iteration {iteration}: Checking lambda = {mid:.4f}\") # Debug print\n",
        "        r_mid = online_risk_lambda(mid,loader, model, device)\n",
        "        print(f\"Iteration {iteration}: Lambda = {mid:.4f}, Risk = {r_mid:.4f}\") # Progress update\n",
        "\n",
        "        if r_mid <= alpha:\n",
        "            right = mid\n",
        "        else:\n",
        "            left = mid\n",
        "        iteration += 1\n",
        "\n",
        "    # The optimal lambda is typically the largest lambda such that R(lambda) <= alpha.\n",
        "    # Due to the nature of binary search and the monotonicity, the right bound\n",
        "    # will be the largest value found that satisfies the condition (within epsilon).\n",
        "    print(f\"Binary search finished. Optimal lambda found: {right:.6f}\")\n",
        "    return right\n",
        "\n",
        "# Now you can call online_binary_search with your model, val_loader, and desired alpha\n",
        "# For example:\n",
        "# alpha_target = 0.1\n",
        "# lambda_hat = online_binary_search(model, val_loader, alpha_target, device=DEVICE)\n",
        "# print(f\"λ̂ obtained: {lambda_hat:.4f}\")"
      ],
      "id": "af72592f",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}